{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.2 64-bit",
   "display_name": "Python 3.8.2 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sample for pytorch forecasting\n",
    "- c.f. https://towardsdatascience.com/introducing-pytorch-forecasting-64de99b9ef46"
   ]
  },
  {
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ],
   "cell_type": "code",
   "metadata": {
    "tags": []
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/fastparquet/parquet_thrift/parquet/ttypes.py:1929: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  iprot._fast_decode(self, iprot, [self.__class__, self.thrift_spec])\n",
      "/usr/local/lib/python3.8/dist-packages/fastparquet/parquet_thrift/parquet/ttypes.py:975: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  iprot._fast_decode(self, iprot, [self.__class__, self.thrift_spec])\n",
      "/usr/local/lib/python3.8/dist-packages/fastparquet/parquet_thrift/parquet/ttypes.py:975: DeprecationWarning: PY_SSIZE_T_CLEAN will be required for '#' formats\n",
      "  iprot._fast_decode(self, iprot, [self.__class__, self.thrift_spec])\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.data.examples import get_stallion_data\n",
    "data = get_stallion_data()  # load data as pandas dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0      2013-01-01\n",
       "7233   2013-02-01\n",
       "9011   2013-03-01\n",
       "Name: date, dtype: datetime64[ns]"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "data.date.drop_duplicates().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "          agency     sku     volume       date  industry_volume  soda_volume  \\\n",
       "291    Agency_25  SKU_03     0.5076 2013-01-01        492612703    718394219   \n",
       "871    Agency_29  SKU_02     8.7480 2015-01-01        498567142    762225057   \n",
       "19532  Agency_47  SKU_01     4.9680 2013-09-01        454252482    789624076   \n",
       "2089   Agency_53  SKU_07    21.6825 2013-10-01        480693900    791658684   \n",
       "9755   Agency_17  SKU_02   960.5520 2015-03-01        515468092    871204688   \n",
       "7561   Agency_05  SKU_03  1184.6535 2014-02-01        425528909    734443953   \n",
       "19204  Agency_11  SKU_05     5.5593 2017-08-01        623319783   1049868815   \n",
       "8781   Agency_48  SKU_04  4275.1605 2013-03-01        509281531    892192092   \n",
       "2540   Agency_07  SKU_21     0.0000 2015-10-01        544203593    761469815   \n",
       "12084  Agency_21  SKU_03    46.3608 2017-04-01        589969396    940912941   \n",
       "\n",
       "       avg_max_temp  price_regular  price_actual    discount  ...  \\\n",
       "291       25.845238    1264.162234   1152.473405  111.688829  ...   \n",
       "871       27.584615    1316.098485   1296.804924   19.293561  ...   \n",
       "19532     30.665957    1269.250000   1266.490490    2.759510  ...   \n",
       "2089      29.197727    1193.842373   1128.124395   65.717978  ...   \n",
       "9755      23.608120    1338.334248   1232.128069  106.206179  ...   \n",
       "7561      28.668254    1369.556376   1161.135214  208.421162  ...   \n",
       "19204     31.915385    1922.486644   1651.307674  271.178970  ...   \n",
       "8781      26.767857    1761.258209   1546.059670  215.198539  ...   \n",
       "2540      28.987755       0.000000      0.000000    0.000000  ...   \n",
       "12084     32.478910    1675.922116   1413.571789  262.350327  ...   \n",
       "\n",
       "       football_gold_cup  beer_capital  music_fest discount_in_percent  \\\n",
       "291                    -             -           -            8.835008   \n",
       "871                    -             -           -            1.465966   \n",
       "19532                  -             -           -            0.217413   \n",
       "2089                   -  beer_capital           -            5.504745   \n",
       "9755                   -             -  music_fest            7.935699   \n",
       "7561                   -             -           -           15.218151   \n",
       "19204                  -             -           -           14.105636   \n",
       "8781                   -             -  music_fest           12.218455   \n",
       "2540                   -             -           -            0.000000   \n",
       "12084                  -             -           -           15.654088   \n",
       "\n",
       "      timeseries time_idx month log_volume avg_volume_by_sku  \\\n",
       "291          228        0     1  -0.678062       1225.306376   \n",
       "871          177       24     1   2.168825       1634.434615   \n",
       "19532        322        8     9   1.603017       2625.472644   \n",
       "2089         240        9    10   3.076505         38.529107   \n",
       "9755         259       26     3   6.867508       2143.677462   \n",
       "7561          21       13     2   7.077206       1566.643589   \n",
       "19204         17       55     8   1.715472       1385.225478   \n",
       "8781         151        2     3   8.360577       1757.950603   \n",
       "2540         300       33    10 -18.420681          0.000000   \n",
       "12084        181       51     4   3.836454       2034.293024   \n",
       "\n",
       "      avg_volume_by_agency  \n",
       "291              99.650400  \n",
       "871              11.397086  \n",
       "19532            48.295650  \n",
       "2089           2511.035175  \n",
       "9755            396.022140  \n",
       "7561           1881.866367  \n",
       "19204           109.699200  \n",
       "8781           1925.272108  \n",
       "2540           2418.719550  \n",
       "12084           109.381800  \n",
       "\n",
       "[10 rows x 31 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>agency</th>\n      <th>sku</th>\n      <th>volume</th>\n      <th>date</th>\n      <th>industry_volume</th>\n      <th>soda_volume</th>\n      <th>avg_max_temp</th>\n      <th>price_regular</th>\n      <th>price_actual</th>\n      <th>discount</th>\n      <th>...</th>\n      <th>football_gold_cup</th>\n      <th>beer_capital</th>\n      <th>music_fest</th>\n      <th>discount_in_percent</th>\n      <th>timeseries</th>\n      <th>time_idx</th>\n      <th>month</th>\n      <th>log_volume</th>\n      <th>avg_volume_by_sku</th>\n      <th>avg_volume_by_agency</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>291</th>\n      <td>Agency_25</td>\n      <td>SKU_03</td>\n      <td>0.5076</td>\n      <td>2013-01-01</td>\n      <td>492612703</td>\n      <td>718394219</td>\n      <td>25.845238</td>\n      <td>1264.162234</td>\n      <td>1152.473405</td>\n      <td>111.688829</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>8.835008</td>\n      <td>228</td>\n      <td>0</td>\n      <td>1</td>\n      <td>-0.678062</td>\n      <td>1225.306376</td>\n      <td>99.650400</td>\n    </tr>\n    <tr>\n      <th>871</th>\n      <td>Agency_29</td>\n      <td>SKU_02</td>\n      <td>8.7480</td>\n      <td>2015-01-01</td>\n      <td>498567142</td>\n      <td>762225057</td>\n      <td>27.584615</td>\n      <td>1316.098485</td>\n      <td>1296.804924</td>\n      <td>19.293561</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>1.465966</td>\n      <td>177</td>\n      <td>24</td>\n      <td>1</td>\n      <td>2.168825</td>\n      <td>1634.434615</td>\n      <td>11.397086</td>\n    </tr>\n    <tr>\n      <th>19532</th>\n      <td>Agency_47</td>\n      <td>SKU_01</td>\n      <td>4.9680</td>\n      <td>2013-09-01</td>\n      <td>454252482</td>\n      <td>789624076</td>\n      <td>30.665957</td>\n      <td>1269.250000</td>\n      <td>1266.490490</td>\n      <td>2.759510</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>0.217413</td>\n      <td>322</td>\n      <td>8</td>\n      <td>9</td>\n      <td>1.603017</td>\n      <td>2625.472644</td>\n      <td>48.295650</td>\n    </tr>\n    <tr>\n      <th>2089</th>\n      <td>Agency_53</td>\n      <td>SKU_07</td>\n      <td>21.6825</td>\n      <td>2013-10-01</td>\n      <td>480693900</td>\n      <td>791658684</td>\n      <td>29.197727</td>\n      <td>1193.842373</td>\n      <td>1128.124395</td>\n      <td>65.717978</td>\n      <td>...</td>\n      <td>-</td>\n      <td>beer_capital</td>\n      <td>-</td>\n      <td>5.504745</td>\n      <td>240</td>\n      <td>9</td>\n      <td>10</td>\n      <td>3.076505</td>\n      <td>38.529107</td>\n      <td>2511.035175</td>\n    </tr>\n    <tr>\n      <th>9755</th>\n      <td>Agency_17</td>\n      <td>SKU_02</td>\n      <td>960.5520</td>\n      <td>2015-03-01</td>\n      <td>515468092</td>\n      <td>871204688</td>\n      <td>23.608120</td>\n      <td>1338.334248</td>\n      <td>1232.128069</td>\n      <td>106.206179</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>music_fest</td>\n      <td>7.935699</td>\n      <td>259</td>\n      <td>26</td>\n      <td>3</td>\n      <td>6.867508</td>\n      <td>2143.677462</td>\n      <td>396.022140</td>\n    </tr>\n    <tr>\n      <th>7561</th>\n      <td>Agency_05</td>\n      <td>SKU_03</td>\n      <td>1184.6535</td>\n      <td>2014-02-01</td>\n      <td>425528909</td>\n      <td>734443953</td>\n      <td>28.668254</td>\n      <td>1369.556376</td>\n      <td>1161.135214</td>\n      <td>208.421162</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>15.218151</td>\n      <td>21</td>\n      <td>13</td>\n      <td>2</td>\n      <td>7.077206</td>\n      <td>1566.643589</td>\n      <td>1881.866367</td>\n    </tr>\n    <tr>\n      <th>19204</th>\n      <td>Agency_11</td>\n      <td>SKU_05</td>\n      <td>5.5593</td>\n      <td>2017-08-01</td>\n      <td>623319783</td>\n      <td>1049868815</td>\n      <td>31.915385</td>\n      <td>1922.486644</td>\n      <td>1651.307674</td>\n      <td>271.178970</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>14.105636</td>\n      <td>17</td>\n      <td>55</td>\n      <td>8</td>\n      <td>1.715472</td>\n      <td>1385.225478</td>\n      <td>109.699200</td>\n    </tr>\n    <tr>\n      <th>8781</th>\n      <td>Agency_48</td>\n      <td>SKU_04</td>\n      <td>4275.1605</td>\n      <td>2013-03-01</td>\n      <td>509281531</td>\n      <td>892192092</td>\n      <td>26.767857</td>\n      <td>1761.258209</td>\n      <td>1546.059670</td>\n      <td>215.198539</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>music_fest</td>\n      <td>12.218455</td>\n      <td>151</td>\n      <td>2</td>\n      <td>3</td>\n      <td>8.360577</td>\n      <td>1757.950603</td>\n      <td>1925.272108</td>\n    </tr>\n    <tr>\n      <th>2540</th>\n      <td>Agency_07</td>\n      <td>SKU_21</td>\n      <td>0.0000</td>\n      <td>2015-10-01</td>\n      <td>544203593</td>\n      <td>761469815</td>\n      <td>28.987755</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>0.000000</td>\n      <td>300</td>\n      <td>33</td>\n      <td>10</td>\n      <td>-18.420681</td>\n      <td>0.000000</td>\n      <td>2418.719550</td>\n    </tr>\n    <tr>\n      <th>12084</th>\n      <td>Agency_21</td>\n      <td>SKU_03</td>\n      <td>46.3608</td>\n      <td>2017-04-01</td>\n      <td>589969396</td>\n      <td>940912941</td>\n      <td>32.478910</td>\n      <td>1675.922116</td>\n      <td>1413.571789</td>\n      <td>262.350327</td>\n      <td>...</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>15.654088</td>\n      <td>181</td>\n      <td>51</td>\n      <td>4</td>\n      <td>3.836454</td>\n      <td>2034.293024</td>\n      <td>109.381800</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 31 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 4
    }
   ],
   "source": [
    "# add time index\n",
    "data[\"time_idx\"] = data[\"date\"].dt.year * 12 + data[\"date\"].dt.month\n",
    "data[\"time_idx\"] -= data[\"time_idx\"].min()\n",
    "# add additional features\n",
    "# categories have to be strings\n",
    "data[\"month\"] = data.date.dt.month.astype(str).astype(\"category\")\n",
    "data[\"log_volume\"] = np.log(data.volume + 1e-8)\n",
    "data[\"avg_volume_by_sku\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"sku\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "data[\"avg_volume_by_agency\"] = (\n",
    "    data\n",
    "    .groupby([\"time_idx\", \"agency\"], observed=True)\n",
    "    .volume.transform(\"mean\")\n",
    ")\n",
    "# we want to encode special days as one variable and \n",
    "# thus need to first reverse one-hot encoding\n",
    "special_days = [\n",
    "    \"easter_day\", \"good_friday\", \"new_year\", \"christmas\",\n",
    "    \"labor_day\", \"independence_day\", \"revolution_day_memorial\",\n",
    "    \"regional_games\", \"fifa_u_17_world_cup\", \"football_gold_cup\",\n",
    "    \"beer_capital\", \"music_fest\"\n",
    "]\n",
    "data[special_days] = (\n",
    "    data[special_days]\n",
    "    .apply(lambda x: x.map({0: \"-\", 1: x.name}))\n",
    "    .astype(\"category\")\n",
    ")\n",
    "# show sample data\n",
    "data.sample(10, random_state=521)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "    easter_day good_friday  new_year christmas labor_day independence_day  \\\n",
       "0            -           -  new_year         -         -                -   \n",
       "238          -           -  new_year         -         -                -   \n",
       "237          -           -  new_year         -         -                -   \n",
       "\n",
       "    revolution_day_memorial regional_games fifa_u_17_world_cup  \\\n",
       "0                         -              -                   -   \n",
       "238                       -              -                   -   \n",
       "237                       -              -                   -   \n",
       "\n",
       "    football_gold_cup beer_capital music_fest  \n",
       "0                   -            -          -  \n",
       "238                 -            -          -  \n",
       "237                 -            -          -  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>easter_day</th>\n      <th>good_friday</th>\n      <th>new_year</th>\n      <th>christmas</th>\n      <th>labor_day</th>\n      <th>independence_day</th>\n      <th>revolution_day_memorial</th>\n      <th>regional_games</th>\n      <th>fifa_u_17_world_cup</th>\n      <th>football_gold_cup</th>\n      <th>beer_capital</th>\n      <th>music_fest</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>-</td>\n      <td>-</td>\n      <td>new_year</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>-</td>\n      <td>-</td>\n      <td>new_year</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>-</td>\n      <td>-</td>\n      <td>new_year</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n      <td>-</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 5
    }
   ],
   "source": [
    "# data[special_days].apply(lambda x: x.map({0: \"-\", 1: x.name})).astype(\"category\")\n",
    "data[special_days].head(3)\n",
    "# .apply(lambda x: print(type(x)))\n",
    "# len(special_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0         48151\n",
       "238       32769\n",
       "237     1219986\n",
       "236      135561\n",
       "235     3044268\n",
       "         ...   \n",
       "6765      71662\n",
       "6764    2180611\n",
       "6763      48146\n",
       "6771    2180611\n",
       "6650    1901290\n",
       "Name: avg_population_2017, Length: 21000, dtype: int64"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "data[\"avg_population_2017\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "        agency     sku\n",
       "0    Agency_22  SKU_01\n",
       "238  Agency_37  SKU_04\n",
       "237  Agency_59  SKU_03"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>agency</th>\n      <th>sku</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Agency_22</td>\n      <td>SKU_01</td>\n    </tr>\n    <tr>\n      <th>238</th>\n      <td>Agency_37</td>\n      <td>SKU_04</td>\n    </tr>\n    <tr>\n      <th>237</th>\n      <td>Agency_59</td>\n      <td>SKU_03</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "data[[\"agency\", \"sku\"]].drop_duplicates().head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "from pytorch_forecasting.data import (\n",
    "    TimeSeriesDataSet,\n",
    "    GroupNormalizer\n",
    ")\n",
    "max_prediction_length = 6  # forecast 6 steps/months\n",
    "max_encoder_length = 24  # use 24 steps/months of history\n",
    "training_cutoff = data[\"time_idx\"].max() - max_prediction_length\n",
    "\n",
    "data_spec = dict(\n",
    "    time_idx=\"time_idx\",\n",
    "    target=\"volume\",\n",
    "    group_ids=[\"agency\", \"sku\"],\n",
    "    target_normalizer=GroupNormalizer(\n",
    "        groups=[\"agency\", \"sku\"], coerce_positive=1.0\n",
    "    ),  # use softplus with beta=1.0 and normalize by group\n",
    "    static_categoricals=[\"agency\", \"sku\"],\n",
    "    static_reals=[\n",
    "        \"avg_population_2017\",\n",
    "        \"avg_yearly_household_income_2017\"\n",
    "    ],\n",
    "    time_varying_known_categoricals=[\"special_days\", \"month\"],\n",
    "    # group of categorical variables can be treated as \n",
    "    # one variable\n",
    "    variable_groups={\"special_days\": special_days},\n",
    "    time_varying_known_reals=[\n",
    "        \"time_idx\",\n",
    "        \"price_regular\",\n",
    "        \"discount_in_percent\"\n",
    "    ],\n",
    "    time_varying_unknown_categoricals=[],\n",
    "    time_varying_unknown_reals=[\n",
    "        \"volume\",\n",
    "        \"log_volume\",\n",
    "        \"industry_volume\",\n",
    "        \"soda_volume\",\n",
    "        \"avg_max_temp\",\n",
    "        \"avg_volume_by_agency\",\n",
    "        \"avg_volume_by_sku\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "preprocess_spec = dict(\n",
    "    add_relative_time_idx=True,  # add as feature\n",
    "    add_target_scales=True,  # add as feature\n",
    "    add_encoder_length=True,  # add as feature\n",
    ")\n",
    "\n",
    "prediction_spec = dict(\n",
    "    min_encoder_length=0,  # allow predictions without history\n",
    "    max_encoder_length=max_encoder_length,\n",
    "    min_prediction_length=1,\n",
    "    max_prediction_length=max_prediction_length,\n",
    ")\n",
    "\n",
    "training = TimeSeriesDataSet(\n",
    "    data[lambda x: x.time_idx <= training_cutoff],\n",
    "    **data_spec,\n",
    "    **preprocess_spec,\n",
    "    **prediction_spec,\n",
    ")\n",
    "# create validation set (predict=True) which means to predict the\n",
    "# last max_prediction_length points in time for each series\n",
    "validation = TimeSeriesDataSet.from_dataset(\n",
    "    training, data, predict=True, stop_randomization=True\n",
    ")\n",
    "# create dataloaders for model\n",
    "batch_size = 128\n",
    "train_dataloader = training.to_dataloader(\n",
    "    train=True, batch_size=batch_size, num_workers=0\n",
    ")\n",
    "val_dataloader = validation.to_dataloader(\n",
    "    train=False, batch_size=batch_size * 10, num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'time_idx': 'time_idx',\n",
       " 'target': 'volume',\n",
       " 'group_ids': ['agency', 'sku'],\n",
       " 'weight': None,\n",
       " 'max_encoder_length': 24,\n",
       " 'min_encoder_length': 24,\n",
       " 'min_prediction_idx': 0,\n",
       " 'min_prediction_length': 1,\n",
       " 'max_prediction_length': 6,\n",
       " 'static_categoricals': ['agency', 'sku'],\n",
       " 'static_reals': ['avg_population_2017',\n",
       "  'avg_yearly_household_income_2017',\n",
       "  'decoder_length',\n",
       "  'volume_center',\n",
       "  'volume_scale'],\n",
       " 'time_varying_known_categoricals': ['special_days', 'month'],\n",
       " 'time_varying_known_reals': ['time_idx',\n",
       "  'price_regular',\n",
       "  'discount_in_percent',\n",
       "  'relative_time_idx'],\n",
       " 'time_varying_unknown_categoricals': [],\n",
       " 'time_varying_unknown_reals': ['volume',\n",
       "  'log_volume',\n",
       "  'industry_volume',\n",
       "  'soda_volume',\n",
       "  'avg_max_temp',\n",
       "  'avg_volume_by_agency',\n",
       "  'avg_volume_by_sku'],\n",
       " 'variable_groups': {'special_days': ['easter_day',\n",
       "   'good_friday',\n",
       "   'new_year',\n",
       "   'christmas',\n",
       "   'labor_day',\n",
       "   'independence_day',\n",
       "   'revolution_day_memorial',\n",
       "   'regional_games',\n",
       "   'fifa_u_17_world_cup',\n",
       "   'football_gold_cup',\n",
       "   'beer_capital',\n",
       "   'music_fest']},\n",
       " 'dropout_categoricals': [],\n",
       " 'constant_fill_strategy': {},\n",
       " 'allow_missings': False,\n",
       " 'add_relative_time_idx': True,\n",
       " 'add_target_scales': True,\n",
       " 'add_encoder_length': True,\n",
       " 'target_normalizer': GroupNormalizer(coerce_positive=1.0, groups=['agency', 'sku'],\n",
       "                 log_zero_value=1.0),\n",
       " 'categorical_encoders': {'month': NaNLabelEncoder(),\n",
       "  'special_days': NaNLabelEncoder(),\n",
       "  'agency': NaNLabelEncoder(),\n",
       "  'sku': NaNLabelEncoder()},\n",
       " 'scalers': {'volume': GroupNormalizer(coerce_positive=1.0, groups=['agency', 'sku'],\n",
       "                  log_zero_value=1.0),\n",
       "  'avg_population_2017': StandardScaler(),\n",
       "  'avg_yearly_household_income_2017': StandardScaler(),\n",
       "  'decoder_length': StandardScaler(),\n",
       "  'volume_center': StandardScaler(),\n",
       "  'volume_scale': StandardScaler(),\n",
       "  'time_idx': StandardScaler(),\n",
       "  'price_regular': StandardScaler(),\n",
       "  'discount_in_percent': StandardScaler(),\n",
       "  'relative_time_idx': StandardScaler(),\n",
       "  'log_volume': StandardScaler(),\n",
       "  'industry_volume': StandardScaler(),\n",
       "  'soda_volume': StandardScaler(),\n",
       "  'avg_max_temp': StandardScaler(),\n",
       "  'avg_volume_by_agency': StandardScaler(),\n",
       "  'avg_volume_by_sku': StandardScaler()},\n",
       " 'randomize_length': None,\n",
       " 'predict_mode': False}"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "training.get_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'reals': tensor([[-0.9593, -0.6123,  0.0000,  ..., -2.9171, -1.0676,  1.0738],\n",
       "         [-0.9593, -0.6123,  0.0000,  ..., -2.1644, -1.0561,  1.3626],\n",
       "         [-0.9593, -0.6123,  0.0000,  ..., -0.9712, -1.0254,  1.6461],\n",
       "         ...,\n",
       "         [ 1.2221,  1.2074,  0.0000,  ..., -0.3227,  1.0434, -1.4095],\n",
       "         [ 1.2221,  1.2074,  0.0000,  ...,  0.0315,  1.5186, -1.4097],\n",
       "         [ 1.2221,  1.2074,  0.0000,  ...,  0.3235,  1.4691, -1.4095]]),\n",
       " 'categoricals': tensor([[ 0,  0,  0,  ...,  0,  0,  0],\n",
       "         [ 0,  0,  0,  ...,  0,  0,  4],\n",
       "         [ 0,  0,  3,  ...,  0,  7,  5],\n",
       "         ...,\n",
       "         [57, 17,  3,  ...,  0,  0,  6],\n",
       "         [57, 17,  0,  ...,  0,  0,  7],\n",
       "         [57, 17,  0,  ...,  0,  0,  8]]),\n",
       " 'groups': tensor([[ 0,  0],\n",
       "         [ 0,  0],\n",
       "         [ 0,  0],\n",
       "         ...,\n",
       "         [57, 17],\n",
       "         [57, 17],\n",
       "         [57, 17]]),\n",
       " 'target': tensor([ 80.6760,  98.0640, 133.7040,  ...,   1.2600,   0.0000,   2.5200]),\n",
       " 'time': tensor([ 0,  1,  2,  ..., 51, 52, 53])}"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "training.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([18900, 2])"
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "training.data[\"reals\"].shape\n",
    "training.data[\"groups\"].shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "\n",
      "   | Name                               | Type                            | Params\n",
      "----------------------------------------------------------------------------------------\n",
      "0  | loss                               | QuantileLoss                    | 0     \n",
      "1  | input_embeddings                   | ModuleDict                      | 1 K   \n",
      "2  | prescalers                         | ModuleDict                      | 256   \n",
      "3  | static_variable_selection          | VariableSelectionNetwork        | 3 K   \n",
      "4  | encoder_variable_selection         | VariableSelectionNetwork        | 8 K   \n",
      "5  | decoder_variable_selection         | VariableSelectionNetwork        | 2 K   \n",
      "6  | static_context_variable_selection  | GatedResidualNetwork            | 1 K   \n",
      "7  | static_context_initial_hidden_lstm | GatedResidualNetwork            | 1 K   \n",
      "8  | static_context_initial_cell_lstm   | GatedResidualNetwork            | 1 K   \n",
      "9  | static_context_enrichment          | GatedResidualNetwork            | 1 K   \n",
      "10 | lstm_encoder                       | LSTM                            | 2 K   \n",
      "11 | lstm_decoder                       | LSTM                            | 2 K   \n",
      "12 | post_lstm_gate_encoder             | GatedLinearUnit                 | 544   \n",
      "13 | post_lstm_add_norm_encoder         | AddNorm                         | 32    \n",
      "14 | static_enrichment                  | GatedResidualNetwork            | 1 K   \n",
      "15 | multihead_attn                     | InterpretableMultiHeadAttention | 1 K   \n",
      "16 | post_attn_gate_norm                | GateAddNorm                     | 576   \n",
      "17 | pos_wise_ff                        | GatedResidualNetwork            | 1 K   \n",
      "18 | pre_output_gate_norm               | GateAddNorm                     | 576   \n",
      "19 | output_layer                       | Linear                          | 119   \n",
      "29625\n",
      "Epoch 0:  97%|█████████▋| 30/31 [00:11<00:00,  2.64it/s, loss=108.328, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 0: 100%|██████████| 31/31 [00:12<00:00,  2.45it/s, loss=108.328, v_num=7]\n",
      "Epoch 1:  97%|█████████▋| 30/31 [00:11<00:00,  2.66it/s, loss=74.439, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 1: 100%|██████████| 31/31 [00:12<00:00,  2.47it/s, loss=74.439, v_num=7]\n",
      "Epoch 2:  97%|█████████▋| 30/31 [00:10<00:00,  2.84it/s, loss=74.052, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 2: 100%|██████████| 31/31 [00:11<00:00,  2.65it/s, loss=74.052, v_num=7]\n",
      "Epoch 3:  97%|█████████▋| 30/31 [00:11<00:00,  2.65it/s, loss=70.372, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 3: 100%|██████████| 31/31 [00:12<00:00,  2.45it/s, loss=70.372, v_num=7]\n",
      "Epoch 4:  97%|█████████▋| 30/31 [00:10<00:00,  2.87it/s, loss=67.753, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 4: 100%|██████████| 31/31 [00:11<00:00,  2.60it/s, loss=67.753, v_num=7]\n",
      "Epoch 5:  97%|█████████▋| 30/31 [00:11<00:00,  2.70it/s, loss=57.997, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 5: 100%|██████████| 31/31 [00:12<00:00,  2.52it/s, loss=57.997, v_num=7]\n",
      "Epoch 6:  97%|█████████▋| 30/31 [00:10<00:00,  2.84it/s, loss=57.980, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 6: 100%|██████████| 31/31 [00:11<00:00,  2.61it/s, loss=57.980, v_num=7]\n",
      "Epoch 7:  97%|█████████▋| 30/31 [00:10<00:00,  2.89it/s, loss=55.589, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 7: 100%|██████████| 31/31 [00:11<00:00,  2.69it/s, loss=55.589, v_num=7]\n",
      "Epoch 8:  97%|█████████▋| 30/31 [00:11<00:00,  2.63it/s, loss=58.633, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 8: 100%|██████████| 31/31 [00:12<00:00,  2.48it/s, loss=58.633, v_num=7]\n",
      "Epoch 9:  97%|█████████▋| 30/31 [00:10<00:00,  2.90it/s, loss=57.843, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 9: 100%|██████████| 31/31 [00:11<00:00,  2.59it/s, loss=57.843, v_num=7]\n",
      "Epoch 10:  97%|█████████▋| 30/31 [00:10<00:00,  2.87it/s, loss=50.610, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 10: 100%|██████████| 31/31 [00:11<00:00,  2.69it/s, loss=50.610, v_num=7]\n",
      "Epoch 11:  97%|█████████▋| 30/31 [00:11<00:00,  2.66it/s, loss=53.407, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 11: 100%|██████████| 31/31 [00:12<00:00,  2.51it/s, loss=53.407, v_num=7]\n",
      "Epoch 12:  97%|█████████▋| 30/31 [00:10<00:00,  2.84it/s, loss=51.741, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 12: 100%|██████████| 31/31 [00:12<00:00,  2.51it/s, loss=51.741, v_num=7]\n",
      "Epoch 13:  97%|█████████▋| 30/31 [00:11<00:00,  2.71it/s, loss=49.936, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 13: 100%|██████████| 31/31 [00:12<00:00,  2.53it/s, loss=49.936, v_num=7]\n",
      "Epoch 14:  97%|█████████▋| 30/31 [00:10<00:00,  2.79it/s, loss=51.120, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 14: 100%|██████████| 31/31 [00:11<00:00,  2.60it/s, loss=51.120, v_num=7]\n",
      "Epoch 15:  97%|█████████▋| 30/31 [00:10<00:00,  2.89it/s, loss=46.684, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 15: 100%|██████████| 31/31 [00:11<00:00,  2.61it/s, loss=46.684, v_num=7]\n",
      "Epoch 16:  97%|█████████▋| 30/31 [00:10<00:00,  2.76it/s, loss=49.002, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 16: 100%|██████████| 31/31 [00:12<00:00,  2.47it/s, loss=49.002, v_num=7]\n",
      "Epoch 17:  97%|█████████▋| 30/31 [00:11<00:00,  2.69it/s, loss=49.900, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 17: 100%|██████████| 31/31 [00:12<00:00,  2.52it/s, loss=49.900, v_num=7]\n",
      "Epoch 18:  97%|█████████▋| 30/31 [00:10<00:00,  2.87it/s, loss=47.589, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 18: 100%|██████████| 31/31 [00:11<00:00,  2.63it/s, loss=47.589, v_num=7]\n",
      "Epoch 19:  97%|█████████▋| 30/31 [00:10<00:00,  2.93it/s, loss=47.679, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 19: 100%|██████████| 31/31 [00:11<00:00,  2.72it/s, loss=47.679, v_num=7]\n",
      "Epoch 20:  97%|█████████▋| 30/31 [00:10<00:00,  2.88it/s, loss=46.591, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 20: 100%|██████████| 31/31 [00:11<00:00,  2.68it/s, loss=46.591, v_num=7]\n",
      "Epoch 21:  97%|█████████▋| 30/31 [00:10<00:00,  2.82it/s, loss=43.866, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 21: 100%|██████████| 31/31 [00:11<00:00,  2.61it/s, loss=43.866, v_num=7]\n",
      "Epoch 22:  97%|█████████▋| 30/31 [00:11<00:00,  2.66it/s, loss=47.059, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 22: 100%|██████████| 31/31 [00:12<00:00,  2.43it/s, loss=47.059, v_num=7]\n",
      "Epoch 23:  97%|█████████▋| 30/31 [00:11<00:00,  2.62it/s, loss=45.670, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 23: 100%|██████████| 31/31 [00:12<00:00,  2.45it/s, loss=45.670, v_num=7]\n",
      "Epoch 24:  97%|█████████▋| 30/31 [00:11<00:00,  2.57it/s, loss=42.433, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 24: 100%|██████████| 31/31 [00:13<00:00,  2.34it/s, loss=42.433, v_num=7]\n",
      "Epoch 25:  97%|█████████▋| 30/31 [00:11<00:00,  2.61it/s, loss=43.675, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 25: 100%|██████████| 31/31 [00:12<00:00,  2.45it/s, loss=43.675, v_num=7]\n",
      "Epoch 26:  97%|█████████▋| 30/31 [00:12<00:00,  2.49it/s, loss=42.563, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 26: 100%|██████████| 31/31 [00:13<00:00,  2.33it/s, loss=42.563, v_num=7]\n",
      "Epoch 27:  97%|█████████▋| 30/31 [00:11<00:00,  2.72it/s, loss=42.087, v_num=7]\n",
      "Validating: 0it [00:00, ?it/s]\u001b[A\n",
      "Epoch 27: 100%|██████████| 31/31 [00:12<00:00,  2.53it/s, loss=42.087, v_num=7]\n",
      "                                                         \u001b[ASaving latest checkpoint..\n",
      "Epoch 27: 100%|██████████| 31/31 [00:12<00:00,  2.40it/s, loss=42.087, v_num=7]\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import (\n",
    "    EarlyStopping,\n",
    "    LearningRateLogger\n",
    ")\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from pytorch_forecasting.metrics import QuantileLoss\n",
    "from pytorch_forecasting.models import TemporalFusionTransformer\n",
    "# stop training, when loss metric does not improve on validation set\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    min_delta=1e-4,\n",
    "    patience=10,\n",
    "    verbose=False,\n",
    "    mode=\"min\"\n",
    ")\n",
    "lr_logger = LearningRateLogger()  # log the learning rate\n",
    "logger = TensorBoardLogger(\"../result/lightning_logs\")  # log to tensorboard\n",
    "# create trainer\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=30,\n",
    "    gpus=0,  # train on CPU, use gpus = [0] to run on GPU\n",
    "    # gpus=[1],  # for GPU\n",
    "    gradient_clip_val=0.1,\n",
    "    early_stop_callback=early_stop_callback,\n",
    "    limit_train_batches=30,  # running validation every 30 batches\n",
    "    # fast_dev_run=True,  # comment in to quickly check for bugs\n",
    "    callbacks=[lr_logger],\n",
    "    logger=logger,\n",
    ")\n",
    "# initialise model\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    training,\n",
    "    learning_rate=0.03,\n",
    "    hidden_size=16,  # biggest influence network size\n",
    "    attention_head_size=1,\n",
    "    dropout=0.1,\n",
    "    hidden_continuous_size=8,\n",
    "    output_size=7,  # QuantileLoss has 7 quantiles by default\n",
    "    loss=QuantileLoss(),\n",
    "    log_interval=10,  # log example every 10 batches\n",
    "    reduce_on_plateau_patience=4,  # reduce learning automatically\n",
    ")\n",
    "print(tft.size())   # 29.6k parameters in model\n",
    "# fit network\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloader=train_dataloader,\n",
    "    val_dataloaders=val_dataloader\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "best_model_path: ../result/lightning_logs/default/version_7/checkpoints/epoch=27.ckpt\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor(264.8931)"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "from pytorch_forecasting.metrics import MAE\n",
    "# load the best model according to the validation loss (given that\n",
    "# we use early stopping, this is not necessarily the last epoch)\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(\"best_model_path:\", best_model_path)\n",
    "best_tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)\n",
    "# calculate mean absolute error on validation set\n",
    "actuals = torch.cat([y for x, y in iter(val_dataloader)])\n",
    "predictions = best_tft.predict(val_dataloader)\n",
    "MAE()(predictions, actuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ]
}